{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbcda827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "import time\n",
    "import threading\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "# from unetlite import UNetLite\n",
    "import fucs  # 你原来的工具函数库（cal_acc 等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceda5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=out_channels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class ClimateEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=10):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 64)\n",
    "        )\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(size=(60, 70), mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Upsample(size=(120, 140), mode='bilinear', align_corners=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.res_blocks(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "class SSTEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.upsample_init = nn.Upsample(size=(100, 180), mode='bilinear', align_corners=True)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.res_block = ResBlock(64, 64)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(size=(60, 70), mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Upsample(size=(120, 140), mode='bilinear', align_corners=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample_init(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.res_block(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa951c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, climate_path, sst_path, precip_path, transform=None):\n",
    "        # ---- Load with mmap to avoid full copy first ----\n",
    "        climate_raw = np.load(climate_path, mmap_mode=\"r\")    # [time, vars, H, W]\n",
    "        sst_raw = np.load(sst_path, mmap_mode=\"r\")            # [time, months, H, W]\n",
    "        precip_raw = np.load(precip_path, mmap_mode=\"r\")      # [time, H, W]\n",
    "\n",
    "        # ---- Standardize climate (per-variable channel) ----\n",
    "        climate_mean = np.nanmean(climate_raw, axis=(0,2,3), keepdims=True)\n",
    "        climate_std  = np.nanstd(climate_raw, axis=(0,2,3), keepdims=True) + 1e-6\n",
    "        climate_stdzd = (climate_raw - climate_mean) / climate_std\n",
    "        # print(\"Standardized climate:\", climate_stdzd.shape, climate_stdzd.dtype, climate_stdzd.nbytes/1e9, \"GB\")\n",
    "\n",
    "\n",
    "        # ---- Standardize sst (per-month channel) ----\n",
    "        sst_filled = np.nan_to_num(sst_raw, nan=0.0).astype(np.float64)\n",
    "        T, M, H, W = sst_filled.shape\n",
    "\n",
    "        sst_mean = np.zeros((1, M, 1, 1), dtype=np.float64)\n",
    "        sst_std  = np.zeros((1, M, 1, 1), dtype=np.float64)\n",
    "\n",
    "        for m in range(M):\n",
    "            # print(f\"Processing month {m}/{M} ...\", flush=True)\n",
    "            channel = sst_filled[:, m, :, :]  # [T,H,W]\n",
    "\n",
    "            # --- 检查异常值 ---\n",
    "            if not np.isfinite(channel).all():\n",
    "                # print(f\"Warning: channel {m} contains NaN/Inf\")\n",
    "                channel = np.nan_to_num(channel, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "            # --- 手写 mean/std 避免 MKL bug ---\n",
    "            count = channel.size\n",
    "            mean_val = channel.sum() / count\n",
    "            var_val = ((channel - mean_val) ** 2).sum() / count\n",
    "            std_val = np.sqrt(var_val) + 1e-6\n",
    "\n",
    "            sst_mean[0, m, 0, 0] = mean_val\n",
    "            sst_std[0, m, 0, 0]  = std_val\n",
    "\n",
    "            # print(f\"month {m}: mean={mean_val:.4f}, std={std_val:.4f}\", flush=True)\n",
    "\n",
    "        # 标准化\n",
    "        sst_stdzd = (sst_filled - sst_mean) / sst_std\n",
    "        # print(\"Standardized sst:\", sst_stdzd.shape, sst_stdzd.dtype, sst_stdzd.nbytes/1e9, \"GB\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---- Standardize precip (global mean/std) ----\n",
    "        precip_mean = np.nanmean(precip_raw)\n",
    "        precip_std  = np.nanstd(precip_raw) + 1e-6\n",
    "        precip_stdzd = (precip_raw - precip_mean) / precip_std\n",
    "        precip_stdzd = np.clip(precip_stdzd, -3.0, 3.0)\n",
    "\n",
    "        # ---- Convert to torch tensors (float32) ----\n",
    "        self.climate_arr = torch.from_numpy(climate_stdzd.astype(np.float32))   # [T, vars, H, W]\n",
    "        self.sst_arr     = torch.from_numpy(sst_stdzd.astype(np.float32))       # [T, months, H, W]\n",
    "        self.precip_arr  = torch.from_numpy(precip_stdzd.astype(np.float32))    # [T, H, W]\n",
    "\n",
    "        # ---- Mask from original precip NaN ----\n",
    "        self.data_mask = torch.from_numpy(np.isnan(precip_raw)).unsqueeze(1)    # [T,1,H,W]\n",
    "        print(\"valid_pixels per sample:\", (~self.data_mask).sum(dim=[1,2,3]).cpu().numpy())\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.precip_arr.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        climate_t = self.climate_arr[idx]\n",
    "        sst_t     = self.sst_arr[idx]\n",
    "        precip_t  = self.precip_arr[idx].reshape(1, *self.precip_arr.shape[1:])\n",
    "        mask_t    = self.data_mask[idx]\n",
    "        if self.transform:\n",
    "            climate_t = self.transform(climate_t)\n",
    "            sst_t     = self.transform(sst_t)\n",
    "        return climate_t, sst_t, precip_t, mask_t\n",
    "\n",
    "# 将 AugmentedSubset 移到模块顶层，避免 Windows multiprocess 无法 pickle 局部类的问题\n",
    "class AugmentedSubset(Dataset):\n",
    "    \"\"\"\n",
    "    对已有 Dataset 的 subset 进行包装，在 __getitem__ 时对 climate/sst 应用 transform（训练增强）。\n",
    "    定义在模块顶层以便 DataLoader 的 worker 可序列化（Windows spawn 模式）。\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.base = base_ds\n",
    "        self.indices = list(indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        climate_t, sst_t, precip_t, mask_t = self.base[idx]\n",
    "        if self.transform is not None:\n",
    "            climate_t = self.transform(climate_t)\n",
    "            sst_t = self.transform(sst_t)\n",
    "        return climate_t, sst_t, precip_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d18e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "valid_pixels per sample: [5301 5301 5301 5301 5301 5301 5301 5301 5301    0    0 5301 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295    0    0 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295    0    0 5295 5237 5237 5237 5237 5237 5237\n",
      " 5237 5237 5237 5237    0 5237 5237 5237 5237 5237 5237 5237 5237 5237\n",
      " 5237 5237    0 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237\n",
      " 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237\n",
      " 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5237 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 6796 6744 5295 5295 5295 5295 5295 5295 5295 5295 5295 7601 7601 7601\n",
      " 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295 5295\n",
      " 5282 5743 5743 5282 5743 5282 5282 5743 5743 5743 5743 5743 5743 5277\n",
      " 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277\n",
      " 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277\n",
      " 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277 5277\n",
      " 5277 5277]\n"
     ]
    }
   ],
   "source": [
    "climate_path = \"E:/D1/diffusion/my_models/mulNet_data/lr.npy\"\n",
    "sst_path = \"E:/D1/diffusion/my_models/mulNet_data/sst.npy\"\n",
    "precip_path = \"E:/D1/diffusion/my_models/mulNet_data/hr.npy\"\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# 先创建一个不带增强的基础数据集（用于验证 & 统计）\n",
    "base_dataset = ClimateDataset(climate_path, sst_path, precip_path, transform=None)\n",
    "n_total = len(base_dataset)\n",
    "n_val = 70\n",
    "n_train = n_total - n_val\n",
    "\n",
    "# 使用模块顶层的 AugmentedSubset（避免在主函数内定义类导致 worker 无法序列化）\n",
    "# 训练集使用增强（GaussianNoise），验证集不使用增强\n",
    "train_dataset = AugmentedSubset(base_dataset, range(0, n_train))\n",
    "val_dataset = torch.utils.data.Subset(base_dataset, range(n_train, n_total))\n",
    "\n",
    "# # 统计仍然基于基础数据集（标准化后的目标）\n",
    "# target_stats(base_dataset, list(range(0, n_train)))\n",
    "# inspect_target_distribution(base_dataset, list(range(0, n_train)), \"train\")\n",
    "# inspect_target_distribution(base_dataset, list(range(n_train, len(base_dataset))), \"val\")\n",
    "\n",
    "config = {\n",
    "    'climate_channels': 10,\n",
    "    'sst_channels': 3,\n",
    "    'latent_channels': 16,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 500,\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 1e-3,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'resume_checkpoint': None,\n",
    "    'train_num_workers': 4,\n",
    "    'val_num_workers': 2,\n",
    "    'prefetch_factor': 2,\n",
    "    'acc_frequency': 10,\n",
    "    'ckpt_dir': './weights'\n",
    "    }\n",
    "\n",
    "train_loader_kwargs = {\n",
    "    'batch_size': config['batch_size'],\n",
    "    'shuffle': True,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': config['train_num_workers'],\n",
    "    'prefetch_factor': config['prefetch_factor']\n",
    "}\n",
    "val_loader_kwargs = {\n",
    "    'batch_size': config['batch_size'],\n",
    "    'shuffle': False,\n",
    "    'pin_memory': True,\n",
    "    'num_workers': config['val_num_workers'],\n",
    "    'prefetch_factor': config['prefetch_factor']\n",
    "}\n",
    "if config['train_num_workers'] > 0:\n",
    "    train_loader_kwargs['persistent_workers'] = True\n",
    "if config['val_num_workers'] > 0:\n",
    "    val_loader_kwargs['persistent_workers'] = True\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, **val_loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df250ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ResBlock (保持不变)\n",
    "# -----------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.GroupNorm(num_groups=8, num_channels=out_channels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Encoder & Decoder\n",
    "# -----------------------------\n",
    "class ClimateEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=10, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1),  # 120x140 -> 60x70\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(32, latent_dim, kernel_size=4, stride=2, padding=1),  # 60x70 -> 30x35\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(latent_dim, latent_dim),\n",
    "            ResBlock(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.res_blocks(x)\n",
    "        return x  # [B, latent_dim, 30, 35]\n",
    "\n",
    "\n",
    "class ClimateDecoder(nn.Module):\n",
    "    def __init__(self, output_channels=10, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 32, kernel_size=4, stride=2, padding=1),  # 30x35 -> 60x70\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1),  # 60x70 -> 120x140\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SSTEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super().__init__()\n",
    "        self.upsample_init = nn.Upsample(size=(100, 180), mode='bilinear', align_corners=True)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        self.res_block = ResBlock(64, 64)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(size=(60, 70), mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Upsample(size=(120, 140), mode='bilinear', align_corners=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample_init(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.res_block(x)\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class SSTDecoder(nn.Module):\n",
    "    def __init__(self, output_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 32, kernel_size=4, stride=2, padding=1),  # 50x90 -> 100x180\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Conv2d(32, output_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Autoencoder Wrappers\n",
    "# -----------------------------\n",
    "class ClimateAE(nn.Module):\n",
    "    def __init__(self, input_channels=10, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = ClimateEncoder(input_channels, latent_dim)\n",
    "        self.decoder = ClimateDecoder(input_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SSTAe(nn.Module):\n",
    "    def __init__(self, input_channels=3, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = SSTEncoder(input_channels)\n",
    "        self.decoder = SSTDecoder(input_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset & DataLoader\n",
    "# -----------------------------\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, climate_path, sst_path, mode=\"climate\"):\n",
    "        self.mode = mode\n",
    "        if mode == \"climate\":\n",
    "            arr = np.load(climate_path, mmap_mode=\"r\")  # [T, vars, H, W]\n",
    "        elif mode == \"sst\":\n",
    "            arr = np.load(sst_path, mmap_mode=\"r\")  # [T, months, H, W]\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'climate' or 'sst'\")\n",
    "        arr = np.nan_to_num(arr).astype(np.float32)\n",
    "        self.arr = torch.from_numpy(arr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.arr[idx]\n",
    "        return x, x  # AE: input = target\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training Loop\n",
    "# -----------------------------\n",
    "def train_autoencoder(model, dataloader, device, epochs=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss={total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Example Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    climate_path = \"E:/D1/diffusion/my_models/mulNet_data/lr.npy\"\n",
    "    sst_path = \"E:/D1/diffusion/my_models/mulNet_data/sst.npy\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Train Climate AE\n",
    "    ds = ClimateDataset(climate_path, sst_path, mode=\"climate\")\n",
    "    dl = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "    model = ClimateAE(input_channels=10)\n",
    "    train_autoencoder(model, dl, device, epochs=10)\n",
    "\n",
    "    # Train SST AE\n",
    "    ds_sst = ClimateDataset(climate_path, sst_path, mode=\"sst\")\n",
    "    dl_sst = DataLoader(ds_sst, batch_size=8, shuffle=True)\n",
    "    model_sst = SSTAe(input_channels=3)\n",
    "    train_autoencoder(model_sst, dl_sst, device, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de50f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 25356, 45244, 40456, 41192) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1285\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     36\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m climate, sst, precip, mask \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     38\u001b[0m     climate, sst, precip, mask \u001b[38;5;241m=\u001b[39m climate\u001b[38;5;241m.\u001b[39mto(device), sst\u001b[38;5;241m.\u001b[39mto(device), precip\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     39\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(climate)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1492\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\nc\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1297\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 25356, 45244, 40456, 41192) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ==== 你给的 ResBlock / ClimateEncoder 代码在这里 ====\n",
    "# (略去重复，假设你已经定义好 ClimateEncoder 和 SSTEncoder)\n",
    "\n",
    "class ClimateModel(nn.Module):\n",
    "    \"\"\" 在 ClimateEncoder 后加一个 1x1 卷积作为预测头 \"\"\"\n",
    "    def __init__(self, input_channels=10):\n",
    "        super().__init__()\n",
    "        self.encoder = ClimateEncoder(input_channels)\n",
    "        self.head = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)  # [B,64,120,140]\n",
    "        out = self.head(feat)   # [B,1,120,140]\n",
    "        return out\n",
    "\n",
    "# ==== 假数据 (替换成你的 DataLoader) ====\n",
    "# B, T, H, W = 16, 10, 120, 140\n",
    "# climate_x = torch.randn(100, T, H, W)\n",
    "# target_y = torch.randn(100, 1, H, W)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, **train_loader_kwargs)\n",
    "\n",
    "# ==== 训练 ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ClimateModel(input_channels=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for climate, sst, precip, mask in train_loader:\n",
    "        climate, sst, precip, mask = climate.to(device), sst.to(device), precip.to(device), mask.to(device)\n",
    "        pred = model(climate)\n",
    "        loss = criterion(pred, precip)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[ClimateEncoder] Epoch {epoch+1}, Loss={total_loss/len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
